%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Semester project, fall term 2014
%   Author: Jakob Ehrl, born 01/24/91
%   Study program: Computer science, MA 1
%   
%   Professor Dr. Francesco Mondada
%   Assistant: Dr. Stefan Witwicki
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Central Processing}

\section{Setup of Processing Units}
Our robot is equipped with four different processing units that, together,
form a network of workers performing individual tasks. There is one master
node that receives sensor readings and lower level computation results from
the other nodes. This data is used to make decisions and finally send 
commands to control the actuators.


\subsection{Master Node}
An Odroid-C1 was used as master node. Similar to the well known Raspberry Pi,
this miniature computer has a Unix-based operating system and possesses numerous
ports such as USB, Ethernet, HDMI and various GPIO pins. We decided to use this
board instead of the Raspberry Pi, which is available in the virtual catalog,
because of the way we intended to design the network of processing units and 
data processing. With four USB ports instead of only two, four CPU cores and twice 
the amount of SDRAM, the Odroid-C1 allows to easily connect the other processing 
boards as well as a camera via USB. This allowed us to develop code in the same
way as if we were working on usual desktop computers. The need for a quad-core 
processor is justified since we aimed at separating the software into different 
modules that are supposed to run independently and in parallel. Pthreads were
used to process data from different inputs independently instead of in a single-loop program.

\subsection{Slaves}
Three Arduino boards were connected to the odroid via serial connection. The following
lists their responsibilities.

\begin{itemize}
    \item Arduino Micro 1
    \subitem control of brush motor
    \subitem data acquisition from five infrared sensors
    
    \item Arduino Micro 2
    \subitem data acquisition from compass 
    \subitem data acquisition from linear cameras

    \item Wild Thumper Controller
    \subitem control of wheels
    \subitem control of tailgate
\end{itemize}

\subsection{Communication}
To transmit data acquired by the Arduino boards and commands generated by the master,
we use the serial communication protocol. The micro controllers do not communicate with
each other. On both sides of the transmission there is a timed control loop that reads
and writes data at a frequency of 30 Hz. To reduce communication costs, all the available
data is first stored in a buffer. Only one message with all the collected commands/sensor
readings is transmitted per interval. This also allows us to filter sensor readings
and reduce communication overhead when the micro controller's control loop is faster 
than the master's planning phase.

\subsubsection{Message Format}
A message is composed of four ASCII characters. The first character being either an
upper- or lowercase character and the following three characters being digits with
leading zeros. Uppercase letters denote command IDs while lowercase letters denote
sensor IDs. Any number between 0 and 999 is allowed as respective value. 

\subsubsection{Limiting Factors}
Since Arduino boards have a small buffer for serial communication (64 bytes), continuous
communication causes a major bottleneck on the micro controllers. Therefore we 
had to limit the update frequency to 30 Hz and also limit the number of messages
that can be sent per iteration.


\section{Master Program Structure}
Code for the master node was written in C++ and designed in a modular way. The bottom
layer includes modules for sensing and acting. A middle layer provides easy access to 
the most recent sensor readings and also an interface for high-level control functions
by hiding actual actuator IDs and value checks behind human readable names such as
\texttt{setWheelSpeeds\{left, right\}}. The top level is where plans are made and 
commands are created. It offers both human interaction via keyboard or autonomous mode
through implementation of classes with access to the middle layer.

\subsection{Bottom Layer}

\subsubsection{Communication Module}
The class \texttt{Serial} implements low level serial communication between the master
node and any other node. The command line options \texttt{ard1, ard2, wtc} specify 
which entity the object shall represent. Their respective serial port locations must
be given as arguments after the option. For testing purposes, the program can be run
with three, two or no slave node connections.

Each connection to a slave is bounded in an object of class \texttt{Serial} having 
its own read and write buffer and its own thread. The communication loop inside a thread
is timed to read and write data at a frequency of 30 Hz to be congruent with the
micro controllers.

\subsubsection{Vision Module}
The class \texttt{Detector} implements the vision part of the robot. An object of this
class can be created from a connected camera or a single image (for development purposes).
It offers two options for image processing both working with color images. 

The first algorithm is based on a K-Means segmentation of the image. \texttt{BGPattern} allows
to create a desired number of color patterns that are later used to segment the image
into background and foreground. Given that there is a very limited number of possible
backgrounds in the arena (tiles, wood, grass, stones), the basic idea was to assign 
pixels within a certain distance to one of the background patterns. Pixels that are too
far from any of the clusters could then be considered as candidates for bottles.
Morphological operations (opening for noise reduction and dilation for region growing)
on the remaining binary image would eventually allow to classify regions of a certain
size and shape as bottles or obstacles.

The second algorithm is also based on the assumption of highly uniform background
patterns. It is implemented by the class \texttt{RangeFinder}. Assuming that the robot
is currently on drivable terrain, a small area in the lower part of the image (directly
in front of the robot) is used as sample pattern. This region is extended toward the
top of the image whilst the change in color is below a certain threshold. Boundaries
of this area are candidates for obstacles, bottles or sudden terrain changes. This 
option was found to generalize better than the first. A more detailed description is
given in the next section.

\subsection{Middle Layer}
The class \texttt{Brain} serves as a wrapper for sensing and acting modules. It 
communicates with objects from the bottom layer (\texttt{Serial, Detector}).
It provides access to all the measurements via the respective IDs defined in \texttt{defines.h}).
Getter and setter functions with respective value range checks allow to use the low-level 
functions from a more abstract point of view.

\subsection{Top Layer}
In order to control the robot either manually or from a custom controller, it is 
necessary to create objects from the lowest level and instantiate an object of type \texttt{Brain}.
An example for keyboard control can be found in \texttt{main.cpp}. Keys are directly mapped
to actions and therefore allow for remote control. Displaying visual output is optional
and can be disabled by the option \texttt{no\_cam}.

For autonomous mode, we tested two different approaches in the classes \texttt{Simulation} and
\texttt{Autonomous}.

\section{Vision}

\subsection{Camera}
A Playstation 3 Camera, an off-the-shelf USB camera, was mounted on a bridge on top of 
the robot covering a field of view from 10~cm to 250~cm in front of the robot and 
widths of 50~cm to 280~cm respectively. This camera was mainly chosen for its easily
modifiable Linux driver and its wide range of manual options such as exposure time,
gain and frame rate. Short exposure times at a constantly high frame rate were needed
to avoid blurred images. Since the camera is mounted almost 50~cm over ground, vibrations
and rough terrain (rocks) might cause blur and thereby make images useless.

\subsection{Algorithms}
Images taken by the camera were not only processed for object detection but also for 
the detection of terrain changes. Since our robot's bottle collection mechanism can only
work when it touches the ground, approaching the grass area or the rock barrier are
a particular danger causing the shovel to hook under the grass carpet or a rock.
The following explains the three key parts of our final algorithm.

\subsubsection{Preprocessing}
The input image is a $320 \times 240$ RGB image. 
An integral image is created from the original frame. Colors are not transformed into
gray scale. Therefore, the resulting integral image is still a three-channel matrix
of size $width + 1 \times height + 1$. The main advantage of integral images is 
that sums over rectangular pixel areas can be computed in constant time. The later
steps take advantage of this fact when computing average colors or applying filters.

\subsubsection{Terrain Segmentation}
Assuming that the image bottom captures drivable terrain, we want to grow determine 
future drivable terrain by growing the region captured in the image bottom toward
the top of the image.

For this purpose, the image is divided into equally spaced columns of 20 pixels width.
In each column, we compute the average color over a region of 20 pixels height, hence 
a block size of $20 \times 20$ pixels. We then move up the image by half a block size
and re-compute the average color. We repeat this step until the distance between the
current and the preceding (half a block size lower) region is above a certain 
threshold. From an abstract point of view, we grow the drivable area by allowing a 
limited change in color. \figurename~\ref{fig:segmentation} shows the robots's point
of view when approaching the elevated area. For visualization the image is masked
with the result of the terrain segmentation step. Black means non-drivable.

\begin{figure}
\center
\subfigure{
\includegraphics[width=0.45\textwidth]{figures/005.png}}
\hfill
\subfigure{
\includegraphics[width=0.45\textwidth]{figures/005-mask.png}}
\label{fig:segmentation}
\caption{Segmentation of drivable terrain.}
\end{figure}

\subsubsection{Detection of Terrain Transitions}
Given that terrain transitions in the arena are sharp straight lines, we can detect
those by fitting a regression line to the result of the segmentation step. Mostly, the
next encountered terrain does not span over the whole image and also only a small portion
of it is interesting to the robot -- the portion directly in front -- since it cannot
displace itself laterally.

Using OpenCV's \texttt{fitLine} function, we estimate a line in
2D over the central ten column heights of the previous step. Using the resulting
slope and offset, we compute the mean squared error of our estimate (summing up the 
squared differences between the true column heights and the predicted ones).
\figurename~\ref{fig:transition} shows the robot's point of view when approaching the
rock barrier with the expected terrain boundary denoted by the regression line in blue.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{figures/terrain-transition.png}
\label{fig:transition}
\caption{Regression line to terrain segmentation.}
\end{figure}

A low error means that the line fitting is appropriate and a terrain change is likely
while a high error signalizes a noisy boundary of the field of view. Boundary interruptions
(some columns lower than others) can be caused by obstacles, bottles or difficult lightning
conditions. Distance measures from the camera can help to detect 
terrain transitions as they cannot be seen by the infrared sensors. Obstacles,
however, can be detected by the infrared sensors. Consequently, range data from 
our infrared sensors should have higher weight then vision data and vision data
shall be used in cases where we have no response from infrared sensors.


\subsubsection{Beacon Detection}


\subsubsection{Bottle Detection}

\subsubsection{Generated Data}
Output data generated from the above vision algorithms is collected in an object of type
\texttt{VisionMeasure} and handed to the \texttt{Brain}. It contains $x-$ and $y-$coordinates
in pixels, where $y=0$ is the bottom pixel row and $x=0$ is the horizontal center of the
image. There is also a vector of potential bottles with their respective coordinates,
slope and intersect of a regression line fit to the ten central bars and an error measure
reflecting confidence in the regression line.

Even though we could work with 3D data by calibrating the camera with respect to the
floor and compute distances and lateral offsets in cm, we decided to work with pixel
coordinates. We tried translating pixel coordinates into cm by measuring the field of view
of the camera and using the results for linear interpolation. However, this is eventually
only a scaling factor that can easier be taken care of when using the vision data. Hence,
we decided to transmit raw pixel measures and adjust accordingly in the control algorithm.

\subsection{Results}

\section{Control Algorithm}
At the moment our robot is a Braitenberg vehicle, meaning that depending on the type
of reading it receives steers its wheels toward or away from the perceived object. 
Terrain classification as well as positions of obstacles and bottles detected through 
vision will be added to guide the robot. Special behaviors as picking up a bottle, 
returning home to the recycling station or releasing loaded bottles are planned.
