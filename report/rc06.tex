%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Semester project, fall term 2014
%   Author: Jakob Ehrl, born 01/24/91
%   Study program: Computer science, MA 1
%   
%   Professor Dr. Francesco Mondada
%   Assistant: Dr. Stefan Witwicki
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Central Processing}

\section{Setup of Processing Units}
Our robot is equipped with four different processing units that, together,
form a network of workers performing individual tasks. There is one master
node that receives sensor readings and lower level computation results from
the other nodes. This data is used to make decisions and finally send 
commands to control the actuators.


\subsection{Master Node}
An Odroid-C1 was used as master node. Similar to the well known Raspberry Pi,
this miniature computer has a Unix-based operating system and possesses numerous
ports such as USB, Ethernet, HDMI and various GPIO pins. We decided to use this
board instead of the Raspberry Pi, which is available in the virtual catalog,
because of the way we intended to design the network of processing units and 
data processing. With four USB ports instead of only two, four CPU cores and twice 
the amount of SDRAM, the Odroid-C1 allows to easily connect the other processing 
boards as well as a camera via USB. This allowed us to develop code in the same
way as if we were working on usual desktop computers. The need for a quad-core 
processor is justified since we aimed at separating the software into different 
modules that are supposed to run independently and in parallel. Pthreads were
used to process data from different inputs independently instead of in a single-loop program.

\subsection{Slaves}
Three Arduino boards were connected to the odroid via serial connection. The following
lists their responsibilities.

\begin{itemize}
    \item Arduino Micro 1
    \subitem control of brush motor
    \subitem data acquisition from five infrared sensors
    
    \item Arduino Micro 2
    \subitem data acquisition from compass 
    \subitem data acquisition from linear cameras

    \item Wild Thumper Controller
    \subitem control of wheels
    \subitem control of tailgate
\end{itemize}

\subsection{Communication}
To transmit data acquired by the Arduino boards and commands generated by the master,
we use the serial communication protocol. The micro controllers do not communicate with
each other. On both sides of the transmission there is a timed control loop that reads
and writes data at a frequency of 30 Hz. To reduce communication costs, all the available
data is first stored in a buffer. Only one message with all the collected commands/sensor
readings is transmitted per interval. This also allows us to filter sensor readings
and reduce communication overhead when the micro controller's control loop is faster 
than the master's planning phase.

\subsubsection{Message Format}
A message is composed of four ASCII characters. The first character being either an
upper- or lowercase character and the following three characters being digits with
leading zeros. Uppercase letters denote command IDs while lowercase letters denote
sensor IDs. Any number between 0 and 999 is allowed as respective value. 

\subsubsection{Limiting Factors}
Since Arduino boards have a small buffer for serial communication (64 bytes), continuous
communication causes a major bottleneck on the micro controllers. Therefore we 
had to limit the update frequency to 30 Hz and also limit the number of messages
that can be sent per iteration.


\section{Master Program Structure}
Code for the master node was written in C++ and designed in a modular way. The bottom
layer includes modules for sensing and acting. A middle layer provides easy access to 
the most recent sensor readings and also an interface for high-level control functions
by hiding actual actuator IDs and value checks behind human readable names such as
\texttt{setWheelSpeeds\{left, right\}}. The top level is where plans are made and 
commands are created. It offers both human interaction via keyboard or autonomous mode
through implementation of classes with access to the middle layer.

\subsection{Bottom Layer}

\subsubsection{Communication Module}
The class \texttt{Serial} implements low level serial communication between the master
node and any other node. The command line options \texttt{ard1, ard2, wtc} specify 
which entity the object shall represent. Their respective serial port locations must
be given as arguments after the option. For testing purposes, the program can be run
with three, two or no slave node connections.

Each connection to a slave is bounded in an object of class \texttt{Serial} having 
its own read and write buffer and its own thread. The communication loop inside a thread
is timed to read and write data at a frequency of 30 Hz to be congruent with the
micro controllers.

\subsubsection{Vision Module}
The class \texttt{Detector} implements the vision part of the robot. An object of this
class can be created from a connected camera or a single image (for development purposes).
It offers two options for image processing both working with color images. 

The first algorithm is based on a K-Means segmentation of the image. \texttt{BGPattern} allows
to create a desired number of color patterns that are later used to segment the image
into background and foreground. Given that there is a very limited number of possible
backgrounds in the arena (tiles, wood, grass, stones), the basic idea was to assign 
pixels within a certain distance to one of the background patterns. Pixels that are too
far from any of the clusters could then be considered as candidates for bottles.
Morphological operations (opening for noise reduction and dilation for region growing)
on the remaining binary image would eventually allow to classify regions of a certain
size and shape as bottles or obstacles.

The second algorithm is also based on the assumption of highly uniform background
patterns. It is implemented by the class \texttt{RangeFinder}. Assuming that the robot
is currently on drivable terrain, a small area in the lower part of the image (directly
in front of the robot) is used as sample pattern. This region is extended toward the
top of the image whilst the change in color is below a certain threshold. Boundaries
of this area are candidates for obstacles, bottles or sudden terrain changes. This 
option was found to generalize better than the first. A more detailed description is
given in the next section.

\subsection{Middle Layer}
The class \texttt{Brain} serves as a wrapper for sensing and acting modules. It 
communicates with objects from the bottom layer (\texttt{Serial, Detector}).
It provides access to all the measurements via the respective IDs defined in \texttt{defines.h}).
Getter and setter functions with respective value range checks allow to use the low-level 
functions from a more abstract point of view.

\subsection{Top Layer}
In order to control the robot either manually or from a custom controller, it is 
necessary to create objects from the lowest level and instantiate an object of type \texttt{Brain}.
An example for keyboard control can be found in \texttt{main.cpp}. Keys are directly mapped
to actions and therefore allow for remote control. Displaying visual output is optional
and can be disabled by the option \texttt{no\_cam}.

For autonomous mode, we tested two different approaches in the classes \texttt{Simulation} and
\texttt{Autonomous}.

\section{Vision}

\subsection{Camera}
A Playstation 3 Camera, an off-the-shelf USB camera, was mounted on a bridge on top of 
the robot covering a field of view from 10~cm to 250~cm in front of the robot and 
widths of 50~cm to 280~cm respectively. This camera was mainly chosen for its easily
modifiable Linux driver and its wide range of manual options such as exposure time,
gain and frame rate. Short exposure times at a constantly high frame rate were needed
to avoid blurred images. Since the camera is mounted almost 50~cm over ground, vibrations
and rough terrain (rocks) might cause blur and thereby make images useless.

\subsection{Terrain Segmentation}
Integral image from color image. Assuming that terrain directly in front of robot is 
drivable, we extend this area while ensuring that the color does not change too much.

\subsection{Object Detection}

\subsection{Light}
Lightning conditions play a decisive roll in most vision applications. In this
particular scenario we are blessed with nearly optimal lighting coming from the
ceiling. Due to the vision algorithm design and manual tuning of camera exposure time
our images do not suffer from saturation.

\section{Control Algorithm}
At the moment our robot is a Braitenberg vehicle, meaning that depending on the type
of reading it receives steers its wheels toward or away from the perceived object. 
Terrain classification as well as positions of obstacles and bottles detected through 
vision will be added to guide the robot. Special behaviors as picking up a bottle, 
returning home to the recycling station or releasing loaded bottles are planned.
